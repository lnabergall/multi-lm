Current Notes:
- Generate sample output
- Try copy task on sequences with same distribution as descriptions or code and use the same truncated backprop method
- Setup Tensorboard
- Start simplifications (e.g. training on only a few description2code examples)

Older Notes:
- use masking + bucketing for batches to do truncated backpropseq

Current Simplifications:
- No duplicated thought vector
- Limited data set
- Single layer
- No bidirectional layers
- No residual connections (through time or layers)
- No attention
- Small hidden state
- Character-level
- 50 step truncated backprop (?)
- Little overlap between sequences ('sparse data')
- No pretraining as a language model (encoder + decoder)
- No character-level CNN on inputs
