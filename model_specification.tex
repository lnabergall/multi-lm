\documentclass[12pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}  
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{titlesec}
\usepackage[backend=bibtex]{biblatex}
\usepackage{hyperref}
\usepackage[]{footmisc}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{algpseudocode}
\usepackage{textcomp}
\usepackage{color}
\titleformat{\section}{\normalsize\scshape\center}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\scshape\center}{\thesubsection}{1em}{}
\makeatletter
\renewcommand\@makefntext[1]{%
    \parindent 1em%
    \@thefnmark.~#1}
\makeatother

\makeatletter
\def\moverlay{\mathpalette\mov@rlay}
\def\mov@rlay#1#2{\leavevmode\vtop{%
   \baselineskip\z@skip \lineskiplimit-\maxdimen
   \ialign{\hfil$\m@th#1##$\hfil\cr#2\crcr}}}
\newcommand{\charfusion}[3][\mathord]{
    #1{\ifx#1\mathop\vphantom{#2}\fi
        \mathpalette\mov@rlay{#2\cr#3}
      }
    \ifx#1\mathop\expandafter\displaylimits\fi}
\makeatother

\newtheorem{definition}{Definition}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{example}[definition]{Example}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\theoremstyle{plain}

\begin{document}
\title{\uppercase{\textbf{\normalsize Something something something...}}}

\author{\small{\textsc{Lukas Nabergall}}}
\date{\small{\textsc{\today}}}
\maketitle

\allowdisplaybreaks


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Initial Idea Overview}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The basic idea is to use a ``massively hierarchical" transfer learning approach to learn better (general) language models with faster training during adaptation to new domains. We use a deep neural network (of some type) as the model. Each layer (or adjacent group of layers) of the network corresponds to a different level in the language hierarchy: the first layer (or group of layers) is learned on a very large dataset containing a large number of samples from every available language type, of every available language, of every available linguistic style, etc.; the second layer is specific to a given language type (and trained on the corresponding restricted data set); the third layer is specific to a given language; the fourth layer is specific to a given linguistic style; etc. The learning task is (something like) to predict the next character (and/or word) in a sequence. 

Three reasons to think transfering from such different languages and, in particular, from such different language types, will be (at least a little) beneficial: 

\begin{itemize}
\item At a high level, even seemingly vastly different language types, e.g. natural languages and programming languages, do have some common features---separation into ``words", similarly structured whitespace and punctuation, some intersections in vocabulary, etc. In particular, a programming language and a natural language are more similar than a programming/natural language and a randomly sampled sequence of characters.
\item We can typically describe a given program using natural language. Although this often assumes a significant amount of background knowledge, it already points to large overlaps in the expressible meanings between the two language types, indicating the existence of some shared features. 
\item It is likely (although currently unverified) that when a human reads and writes, say, natural language and programming language, they are using some common parts of the brain. Although the intersection between the parts of the brain used for each of the tasks may be small, this also strongly suggests the existence of common features between the two language types (in particular, that a neural network should likely be able to learn high-level representations useful to modeling/processing both language types). 
\end{itemize}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Ideas}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Some Rough Ideas:

\begin{enumerate}
\item[1.] Recognizing/generating ``comprehensible" natural language
\item[2.] Recognizing/generating/classifying any ``structured" language (generally, English, Chinese, HTML, C, LaTeX, Morse, etc.)
\item[3.] Converting between linguistic styles (e.g. academic $\leftrightarrow$ Trump $\leftrightarrow$ Shakespeare $\leftrightarrow$ Twitter)
\item[4.] Correcting informal/sloppy/incorrect natural language (or, generally, correcting any "structured" language) --- more specifically, domain-specific natural language correction (e.g. correcting text from/for Twitter, Wikipedia, a Trump speech, a Senate bill; more generally, HTML, C, LaTeX, Morse, Chinese, etc.)
\end{enumerate}

%%% I was thinking about using as largely and varied as possible a data set (10s-100s of billions of words or more) from many sources to train a "background" network, with the idea being replicating a person's ability to understand "arbitrary" text








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{enumerate}
\item Plank, What to do about non-standard (or non-canonical) language in NLP, (2016).
\item Jozefowicz, Vinyals, Schuster, Shazeer, Wu, Exploring the Limits of Language Modeling, (2016).
\item Zhang, Liu, Wang, Zhu, Neural Personalized Response Generation as Domain Adaptation, (2017).
\item Wang and Zheng, Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding, (2015).
\item Wang and Zheng, Transfer Learning for Speech and Language Processing, (2015).
\item Yoon, Yun, Kim, Park, Jung, Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network, (2017).
\item Yosinski, Clune, Bengio, Lipson, How transferable are features in deep neural networks, (2014).
\item Shazeer, Mirgoseini, Maziarz, Davis, Le, Hinton, Dean, Outrageously large neural networks: The Sparsely-Gated Mixture-of-Experts Layer, (2017).
\item Dam, Tran, Pham, A deep language model for software code, (2016).
\item Ruder, Ghaffari, Breslin, Character-level and Multi-channel Convolutional Neural Networks for Large-scale Authorship Attribution, (2016).
\item Kim, Jernite, Sontag, Rush, Character-aware Neural Language Models, (2015).
\item Krause, Lu, Murray, Renals, Multiplicative LSTM for sequence modelling, (2016).
\item Wu et al., Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation, (2016). \label{Wu2016}
\item Jozefowicz, Zaremba, Sutskever, An Empirical Exploration of Recurrent Network Architectures, (2015). \label{Jozefowicz2015}
\end{enumerate}

%%% Note: using wiktionary (or generally a dictionary/thesaurus) as supporting data







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Some notes:
%%%
%%% Incorporating character-level inputs will likely be a good/necessary idea for most problems under consideration
%%%
%%% Consider applying convolutional and generative adversarial networks, and reinforcement learning
%%%
%%% Perhaps use a very high dropout probability to increase training speed, even for e.g. extremely large data sets or very large networks

\begin{enumerate}
\item[] Task
\begin{enumerate}
\item[--] Develop a single character-level language model for many different ``specific" languages\footnote{Essentially defined as a subspace of a language (which itself is the space of ``all possible" texts in that language) with a given style and written by a single author.} with simultaneous training and the ability to quickly transfer learn other specific languages.
\item[--] Formally, given a sequence of words $w_{1}, \ldots, w_{n}$ with character representations $w_{i} = (c_{i_{1}}, \ldots, c_{i_{k}})$ and a 4-tuple $(L_{1}, L_{2}, L_{3}, L_{4})$ (specifying, respectively, the language type, language, language style, and author of the word sequence), the language model estimates joint probabilities using the chain rule:
$$p(w_{1}, \ldots, w_{n}) = p(w_{1})\prod_{i=2}^{n}p(w_{i} \mid w_{1}, \ldots, w_{i-1}).$$
\end{enumerate}
\item[] Base Model
\begin{enumerate}
\item[--] A ``branching" 5-layer long short-term memory network which, at each time step, processes an input character sequence $(c_{i_{1}}, \ldots, c_{i_{k}})$ for a word $w_{i}$ using a character-level convolutional neural network and outputs $p(w_{i} \mid w_{1}, \ldots, w_{i-1})$ using a softmax output layer. 
\item[--] Each ``branch" at the $i$-th layer in the network takes as input the hidden state from the $(i-1)$-th layer. The $i$-th layer processes input text at the $i$-th level of language diversity: all text is processed by the single branch in the first layer, then text in each language type is processed by a distinct branch in the second layer, text in each language is processed by a distinct branch in the third layer, text in each language style is processed by a distinct branch in the fourth layer, and finally text written by each author is processed by a distinct branch in the fifth layer. 
\end{enumerate}
\item[] Possible Model Modifications/Improvements
\begin{enumerate}
\item[--] Connections between additional previous time steps to increase the network's ability to learn very long-term dependencies---instead of only passing in the cell state and hidden state from the previous time step to the current time step, we pass in the cell state and/or hidden state of a subset of all the previous timesteps (e.g. the previous $n$ time steps). 
\item[--] Residual connections between layers to improve training of the deep LSTM---instead of passing in the hidden state to the next layer, the hidden state plus this layer's input is passed to the next layer (e.g. [\ref{Wu2016}]). 
\item[--] Initialize the bias of the forget gate to $\geqslant 1$ to help prevent the vanishing gradient problem [\ref{Jozefowicz2015}].
\item[--] 
\item[--] 
\end{enumerate}
\item[] Data
\begin{enumerate}
\item[--] There are many different language spaces\footnote{That is, for a given alphabet.}, types of languages, languages, language styles, and authors---in particular, there are thousands (or more) in the last three categories. For tractability, we restrict to a small number of the most common from each category. On the other hand, to maximize the benefits of simultaneous ``transfer" learning, we will likely want to use at least 3 elements from each category at each language diversity level. The most important considerations when choosing language types, languages, etc. are diversity and ``popularity", both of which should be maximized. 
\item[--] For the language space, we choose only one, and there are two possibilities: (1) Roman alphabet languages\footnote{We may restrict this to be languages that use the alphabet used by Western European and American languages, or consider more broadly all Roman alphabet languages (which would include many more letters and languages).}, (2) all languages expressible in unicode. For language type, we will work with the 3 most relevant and commonly used:\footnote{Probably.} natural language, programming language, and markup language. For languages, there are in general many possible choices per language type---perhaps: English, French, and German (natural language);\footnote{Other possibilities: Spanish, Portuguese, Danish, Swedish, Norwegian, etc.} C, Python, and Lisp (programming language);\footnote{Other possibilities: Java, C++, C\#, Fortran, JavaScript, Pascal, Perl, Ruby, Visual Basic, Go, etc.} HTML, \LaTeX, YAML/Markdown/ASN.1/RTF (markup language).
\item[--] 
\item[--] 
\end{enumerate}
\item[] Training
\begin{enumerate}
\item[--] 
\item[--] 
\item[--] 
\item[--] 
\end{enumerate}
\item[] Testing/Evaluation
\begin{enumerate}
\item[--] 
\item[--] 
\item[--] 
\item[--] 
\end{enumerate}
\end{enumerate}






\end{document} 